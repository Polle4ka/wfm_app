import streamlit as st
import pandas as pd
import numpy as np
from dateutil.relativedelta import relativedelta
import altair as alt
import datetime

# ===== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã =====
st.set_page_config(page_title="WFM: –ü—Ä–æ–≥–Ω–æ–∑ –∏ —Ä–∞—Å—á—ë—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤", layout="wide")

# ===== –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è =====
st.markdown("""
<style>
    .stDataFrame { border-radius: 12px; }
</style>
""", unsafe_allow_html=True)

# ===== –ü–µ—Ä–µ–≤–æ–¥ –¥–Ω–µ–π –Ω–µ–¥–µ–ª–∏ =====
weekday_map = {
    "Monday": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "Tuesday": "–í—Ç–æ—Ä–Ω–∏–∫",
    "Wednesday": "–°—Ä–µ–¥–∞",
    "Thursday": "–ß–µ—Ç–≤–µ—Ä–≥",
    "Friday": "–ü—è—Ç–Ω–∏—Ü–∞",
    "Saturday": "–°—É–±–±–æ—Ç–∞",
    "Sunday": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ"
}

# ===== –†–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è =====
WORK_START = 9
WORK_END_EXCL = 20
WORK_HOURS = list(range(WORK_START, WORK_END_EXCL))
SLOTS_PER_DAY = len(WORK_HOURS)
PEAK_HOURS = [9, 10]

# ===== –°–∞–π–¥–±–∞—Ä =====
st.sidebar.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã")
horizon_days = st.sidebar.slider("–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ (–¥–Ω–µ–π)", 1, 31, 7, 1)
efficiency = st.sidebar.number_input("–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–¥–æ–ª—è)", 0.10, 1.0, 0.85, 0.01, format="%.2f")

# AHT —Ä—É—á–Ω–æ–π –≤–≤–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –º–º:—Å—Å
aht_str = st.sidebar.text_input("AHT (–º–º:—Å—Å)", "03:00")
try:
    mm, ss = map(int, aht_str.split(":"))
    aht_min = mm + ss/60
except:
    st.sidebar.error("–í–≤–µ–¥–∏—Ç–µ –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ –º–º:—Å—Å")
    aht_min = 3.0

# –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞
default_start = datetime.date(datetime.date.today().year, 8, 15)
start_date = st.sidebar.date_input("üìÖ –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞", default_start)

# –î–∞—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—Ñ–∞–∫—Ç vs –ø—Ä–æ–≥–Ω–æ–∑)
compare_date = st.sidebar.date_input("üìÖ –î–∞—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑)", None)

# ===== –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö (Google Sheets) =====
url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vRUo-9C-CLiNCnB9CfTXphTbrPknJjjrgogM3dqruy66S6_oKyoeiSUHe0hPMClIGmcmyqLwsYgyupG/pub?gid=1587628005&single=true&output=csv"

@st.cache_data
def load_data():
    df = pd.read_csv(url)
    df["–î–∞—Ç–∞"] = pd.to_datetime(df["–î–∞—Ç–∞"], dayfirst=True, errors="coerce")
    df["–ß–∞—Å"] = pd.to_numeric(df["–ß–∞—Å"], errors="coerce")
    df["–ó–≤–æ–Ω–∫–∏"] = pd.to_numeric(df["–ó–≤–æ–Ω–∫–∏"], errors="coerce")

    def parse_aht(val):
        if pd.isna(val): return np.nan
        try:
            parts = str(val).split(":")
            if len(parts) == 2:
                m, s = parts; return int(m) + int(s)/60
            return float(val)
        except:
            return np.nan

    df["–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º–∏–Ω)"] = df["–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º–∏–Ω)"].apply(parse_aht)
    df = df.dropna(subset=["–î–∞—Ç–∞", "–ß–∞—Å", "–ó–≤–æ–Ω–∫–∏"])
    df["–ß–∞—Å"] = df["–ß–∞—Å"].astype(int)
    df["Datetime"] = pd.to_datetime(df["–î–∞—Ç–∞"].dt.strftime("%Y-%m-%d") + " " + df["–ß–∞—Å"].astype(str) + ":00:00")
    df = df.set_index("Datetime").sort_index()
    df = df[df["–ß–∞—Å"].isin(WORK_HOURS)]
    if "–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏" not in df.columns:
        df["–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏"] = df.index.day_name().map(weekday_map)
    return df

df = load_data()

# ===== –£—Ç–∏–ª–∏—Ç—ã =====
def format_aht(minutes: float):
    if pd.isna(minutes): return "‚Äî"
    m = int(minutes)
    s = int(round((minutes - m) * 60))
    return f"{m:02d}:{s:02d}"

def dates_where_month_ahead_is_weekend(dates) -> set:
    dt = pd.to_datetime(dates)
    uniq = sorted(set(dt.date if isinstance(dt, pd.DatetimeIndex) else dt.dt.date))
    res = set()
    for d in pd.to_datetime(uniq):
        future = d + relativedelta(months=+1)
        if future.weekday() in (5, 6):
            res.add(d.date())
    return res

def generate_future_business_hours(start_exclusive: pd.Timestamp, periods: int) -> pd.DatetimeIndex:
    out, cur = [], start_exclusive
    for _ in range(periods):
        nxt = cur + pd.Timedelta(hours=1)
        if nxt.hour >= WORK_END_EXCL or nxt.hour < WORK_START:
            nxt = (pd.Timestamp(cur.date()) + pd.Timedelta(days=1)).replace(hour=WORK_START)
        out.append(nxt)
        cur = nxt
    return pd.DatetimeIndex(out)

def trimmed_mean(series, lower=0.1, upper=0.9):
    if series.empty: return np.nan
    q_low, q_high = series.quantile([lower, upper])
    return series[(series >= q_low) & (series <= q_high)].mean()

# ===== –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ =====
def build_forecast(base_df, horizon_days, efficiency, aht_min, ref_date=None):
    today = pd.Timestamp.today().normalize()
    if ref_date:
        ref_date = pd.Timestamp(ref_date)

    if ref_date and ref_date < today:
        start_forecast = ref_date.replace(hour=WORK_START)
    else:
        start_forecast = (today + pd.Timedelta(days=1)).replace(hour=WORK_START)

    df_win = base_df[base_df.index >= pd.Timestamp(start_date)]
    if df_win.empty:
        return pd.DataFrame()

    df_win["weekday"] = df_win.index.weekday
    df_win["hour"] = df_win["–ß–∞—Å"]
    pivot_med = df_win.pivot_table(index="hour", columns="weekday", values="–ó–≤–æ–Ω–∫–∏", aggfunc="median")

    opening_days_hist = dates_where_month_ahead_is_weekend(df_win.index)
    is_open_hist = pd.Series(df_win.index.date, index=df_win.index).isin(opening_days_hist)
    is_weekday_hist = df_win.index.weekday < 5

    hour_coefs = {}
    for h in PEAK_HOURS:
        open_vals = df_win[(df_win["hour"] == h) & is_open_hist]["–ó–≤–æ–Ω–∫–∏"]
        base_vals = df_win[(df_win["hour"] == h) & (~is_open_hist) & is_weekday_hist]["–ó–≤–æ–Ω–∫–∏"]
        if len(open_vals) >= 2 and len(base_vals) >= 2 and base_vals.median() > 0:
            coef = trimmed_mean(open_vals) / trimmed_mean(base_vals)
            coef = float(np.clip(coef, 2.0, 5.0))
        else:
            coef = 2.0
        hour_coefs[h] = coef

    steps = horizon_days * SLOTS_PER_DAY
    future_idx = generate_future_business_hours(start_forecast - pd.Timedelta(hours=1), steps)

    base_vals = []
    for t in future_idx:
        w, h = t.weekday(), t.hour
        v = pivot_med.loc[h, w] if (h in pivot_med.index and w in pivot_med.columns) else np.nan
        if pd.isna(v): v = df_win.groupby("hour")["–ó–≤–æ–Ω–∫–∏"].median().get(h, df_win["–ó–≤–æ–Ω–∫–∏"].median())
        base_vals.append(v)

    out = pd.DataFrame({"Datetime": future_idx, "–ë–∞–∑–∞": base_vals})
    out["Date"] = out["Datetime"].dt.strftime("%d-%m-%Y")
    out["Hour"] = out["Datetime"].dt.hour
    out["–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏"] = out["Datetime"].dt.day_name().map(weekday_map)
    out["–û—Ç–∫—Ä—ã—Ç–∏–µ?"] = out["Datetime"].dt.date.isin(dates_where_month_ahead_is_weekend(out["Datetime"]))

    out["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"] = out["–ë–∞–∑–∞"]
    for h in PEAK_HOURS:
        mask = (out["–û—Ç–∫—Ä—ã—Ç–∏–µ?"]) & (out["Hour"] == h)
        out.loc[mask, "–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"] *= hour_coefs[h]

    hist_max = df_win["–ó–≤–æ–Ω–∫–∏"].max()
    out["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"] = np.minimum(out["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"], hist_max * 2.5)
    out["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"] = np.maximum(np.round(out["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"]), 0).astype(int)

    out["–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏"] = np.ceil((out["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"] * aht_min) / (60.0 * efficiency)).astype(int)

    return out.sort_values("Datetime").reset_index(drop=True)

# ===== MAIN =====
st.markdown("<h1>üìû WFM-–º–æ–¥—É–ª—å</h1>", unsafe_allow_html=True)

# –ò—Å—Ç–æ—Ä–∏—è
with st.expander("üìÖ –ò—Å—Ç–æ—Ä–∏—è –ø–æ –¥–Ω—è–º"):
    daily = df.groupby(df["–î–∞—Ç–∞"].dt.date).agg({"–ó–≤–æ–Ω–∫–∏": "sum", "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º–∏–Ω)": "mean"}).reset_index()
    daily["–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (AHT)"] = daily["–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º–∏–Ω)"].apply(format_aht)
    daily["–ú–µ—Å—è—Ü"] = pd.to_datetime(daily["–î–∞—Ç–∞"]).dt.strftime("%B %Y")
    daily["–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏"] = pd.to_datetime(daily["–î–∞—Ç–∞"]).dt.day_name().map(weekday_map)
    sel_month = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Å—è—Ü", sorted(daily["–ú–µ—Å—è—Ü"].unique()))
    daily_month = daily[daily["–ú–µ—Å—è—Ü"] == sel_month]
    st.dataframe(daily_month[["–î–∞—Ç–∞","–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏","–ó–≤–æ–Ω–∫–∏","–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (AHT)"]].reset_index(drop=True),
                 use_container_width=True, height=400)

# –ü—Ä–æ–≥–Ω–æ–∑
forecast = build_forecast(df, horizon_days, efficiency, aht_min)

st.header("üîÆ –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ –¥–Ω—è–º")
for d, grp in forecast.groupby("Date"):
    with st.expander(f"{d} ({grp['–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏'].iloc[0]})"):
        total_calls = grp["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"].sum()
        st.markdown(f"**–í—Å–µ–≥–æ –∑–≤–æ–Ω–∫–æ–≤: {total_calls}**")
        st.dataframe(grp[["Hour", "–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏"]].reset_index(drop=True),
                     use_container_width=True, height=400)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç vs –ø—Ä–æ–≥–Ω–æ–∑
if compare_date:
    compare_date = pd.Timestamp(compare_date)
    st.header(f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∞: {compare_date.date()}")
    fact = df[df["–î–∞—Ç–∞"].dt.date == compare_date.date()]
    pred = build_forecast(df, 1, efficiency, aht_min, ref_date=compare_date)
    if not fact.empty and not pred.empty:
        merged = pd.merge(fact.reset_index(), pred, left_on="–ß–∞—Å", right_on="Hour", how="outer")
        merged["–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (AHT)"] = merged["–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º–∏–Ω)"].apply(format_aht)
        merged["% –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è"] = ((merged["–ó–≤–æ–Ω–∫–∏"] - merged["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"]) / merged["–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤"] * 100).round(1)
        merged = merged[["–ß–∞—Å", "–ó–≤–æ–Ω–∫–∏", "–ü—Ä–æ–≥–Ω–æ–∑ –∑–≤–æ–Ω–∫–æ–≤", "% –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è", "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (AHT)", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏"]]
        st.dataframe(merged.reset_index(drop=True), use_container_width=True, height=400)
    else:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")
